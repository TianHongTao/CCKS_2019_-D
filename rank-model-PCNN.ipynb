{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关系标签\n",
    "+ NA\t0\n",
    "+ 人物关系/亲属关系/配偶/丈夫/现夫\t1\n",
    "+ 人物关系/亲属关系/配偶/丈夫/前夫\t2\n",
    "+ 人物关系/亲属关系/配偶/丈夫/未婚夫\t3\n",
    "+ 人物关系/亲属关系/配偶/妻子/现妻\t4\n",
    "+ 人物关系/亲属关系/配偶/妻子/前妻\t5\n",
    "+ 人物关系/亲属关系/配偶/妻子/未婚妻\t6\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/祖父母/爷爷\t7\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/祖父母/奶奶\t8\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/外祖父母/外公\t9\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/父母/生父\t10\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/父母/生母\t11\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/子女/儿子\t12\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/子女/女儿\t13\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/孙子女/孙子\t14 \n",
    "+ 人物关系/亲属关系/血亲/自然血亲/孙子女/孙女\t15\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/兄弟姐妹/哥哥\t16\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/兄弟姐妹/弟弟\t17\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/兄弟姐妹/姐姐\t18\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/兄弟姐妹/妹妹\t19\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/叔伯姑舅姨/叔伯\t20\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/叔伯姑舅姨/舅舅\t21\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/叔伯姑舅姨/姑妈\t22\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/侄甥/侄子\t23\n",
    "+ 人物关系/亲属关系/血亲/自然血亲/侄甥/侄女\t24\n",
    "+ 人物关系/亲属关系/姻亲/血亲的配偶/子女的配偶/儿媳\t25\n",
    "+ 人物关系/亲属关系/姻亲/血亲的配偶/子女的配偶/女婿\t26\n",
    "+ 人物关系/亲属关系/姻亲/血亲的配偶/兄弟姐妹的配偶/嫂子\t27\n",
    "+ 人物关系/亲属关系/姻亲/配偶的血亲/配偶的父母/公公\t28\n",
    "+ 人物关系/亲属关系/姻亲/配偶的血亲/配偶的父母/岳父\t29\n",
    "+ 人物关系/社交关系/友谊关系/朋友\t30\n",
    "+ 人物关系/社交关系/感情关系/喜欢\t31\n",
    "+ 人物关系/社交关系/感情关系/恋人\t32\n",
    "+ 人物关系/师生关系/老师\t33\n",
    "+ 人物关系/师生关系/学生\t34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn import preprocessing\n",
    "from random import shuffle\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 401\n",
    "LABELNUMS = 36\n",
    "SENTEN_LEN = 50\n",
    "CLASSIFY_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(data1):\n",
    "    return (np.arange(LABELNUMS)==data1[:,None]).astype(np.integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/Users/denhiroshi/Downloads/model_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.from_numpy(np.load(PATH+\"train_Sentense_np.npy\").astype(\"float32\"))\n",
    "train_Y = np.load(PATH+\"train_Relation_np.npy\").astype(\"int32\")\n",
    "dev_X = torch.from_numpy(np.load(PATH+\"dev_Sentense_np.npy\").astype(\"float32\"))\n",
    "dev_Y = np.load(PATH+\"dev_Relation_np.npy\").astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = train_Y.tolist()\n",
    "dev_Y = dev_Y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43501, 35])\n",
      "torch.Size([38417, 35])\n"
     ]
    }
   ],
   "source": [
    "# OneHot\n",
    "train_ontHot_y = []\n",
    "dev_ontHot_y = []\n",
    "for trainY in train_Y:\n",
    "    tmp = np.eye(LABELNUMS)[trainY]\n",
    "    tmp = tmp.sum(axis = 0).astype(\"int32\")\n",
    "    tmp = tmp[:-1]\n",
    "    train_ontHot_y.append(tmp)\n",
    "train_ontHot_y = torch.Tensor(train_ontHot_y)\n",
    "print(train_ontHot_y.shape)\n",
    "for devY in dev_Y:\n",
    "    tmp = np.eye(LABELNUMS)[devY]\n",
    "    tmp = tmp.sum(axis = 0).astype(\"int32\")\n",
    "    tmp = tmp[:-1]\n",
    "    dev_ontHot_y.append(tmp)\n",
    "dev_ontHot_y = torch.Tensor(dev_ontHot_y)\n",
    "print(dev_ontHot_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TEXTCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TEXTCNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(  # input shape (1, BATCH_SIZE, 200)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=(1,202),\n",
    "            ), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(50,1)), \n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential( \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=(3,202), \n",
    "            ), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=(48,1)), \n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential( \n",
    "            nn.Conv2d(\n",
    "                in_channels=1, \n",
    "                out_channels=16, \n",
    "                kernel_size=(5,202), \n",
    "                stride = 1,\n",
    "            ), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(46,1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):        \n",
    "        x = x.reshape((-1,1,x.shape[1],x.shape[2]))\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        x3 = self.conv3(x)\n",
    "        x = torch.cat((x1,x2,x3),1)\n",
    "        x = x.view(x.size(0), -1)   # 展平\n",
    "        # torch.Size([512, 48])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_LSTM_max(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bi_LSTM_max,self).__init__()\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size = 256,       # rnn input\n",
    "            hidden_size = 128,     # rnn hidden unit\n",
    "            num_layers = 3,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "            bidirectional=True\n",
    "        )\n",
    "#         self.maxpool = torch.nn.MaxPool2d((SENTEN_LEN,1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        output,(h_n,c_n)=self.rnn(x.float())\n",
    "#         output_in_last_timestep = self.maxpool(output)\n",
    "#         output_in_last_timestep = output_in_last_timestep.reshape((output_in_last_timestep.shape[0],-1))\n",
    "        # torch.Size([512, 256])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size = 202,       # rnn input\n",
    "            hidden_size = 128,     # rnn hidden unit\n",
    "            num_layers = 3,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "            bidirectional=True\n",
    "        )\n",
    "#         self.maxpool = torch.nn.MaxPool2d((SENTEN_LEN,1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        output,(h_n,c_n)=self.rnn(x.float())\n",
    "#         output_in_last_timestep = self.maxpool(output)\n",
    "#         output_in_last_timestep = output_in_last_timestep.reshape((output_in_last_timestep.shape[0],-1))\n",
    "        # torch.Size([512, 256])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_GRU_max(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size = 256,       # rnn input\n",
    "            hidden_size = 128,     # rnn hidden unit\n",
    "            num_layers = 3,       # 有几层 RNN layers\n",
    "            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.maxpool = torch.nn.MaxPool2d((SENTEN_LEN,1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        output,h_n=self.rnn(x.float())\n",
    "        output_in_last_timestep = self.maxpool(output)\n",
    "        output_in_last_timestep = output_in_last_timestep.reshape((output_in_last_timestep.shape[0],-1))\n",
    "        # torch.Size([512, 256])\n",
    "        return output_in_last_timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PCNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(  # input shape (1, 10, 200)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,      # input height\n",
    "                out_channels=50,    # n_filters\n",
    "                kernel_size=(1,256),      # filter size\n",
    "            ),      # output shape (16, 53, 200)\n",
    "            nn.ReLU(),    # activation\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(SENTEN_LEN,1))   # 在 2x2 空间里向下采样, output shape (16, 53, 1)\n",
    "\n",
    "    def forward(self, x, x_pos):\n",
    "        pos = []\n",
    "#         for i in range(x.shape[0]):\n",
    "#             tmp1 = (x[i,:,-2] == 0).nonzero()[0]\n",
    "#             tmp2 = (x[i,:,-1] == 0).nonzero()[0]\n",
    "#             tmp = torch.cat((tmp1,tmp2))\n",
    "#             pos.append(tmp)\n",
    "        for i in range(x_pos.shape[0]):\n",
    "            tmp1 = (x_pos[i,:,-2] == 0).nonzero()[0]\n",
    "            tmp2 = (x_pos[i,:,-1] == 0).nonzero()[0]\n",
    "            tmp = torch.cat((tmp1,tmp2))\n",
    "            pos.append(tmp)\n",
    "        pos = torch.stack(pos)\n",
    "        x = x.reshape((-1,1,SENTEN_LEN,x.shape[-1]))\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x1 = torch.zeros(x.shape)\n",
    "        x2 = torch.zeros(x.shape)\n",
    "        x3 = torch.zeros(x.shape)\n",
    "        \n",
    "        min_x1 = torch.ones(x.shape) * (-100)\n",
    "        min_x2 = torch.ones(x.shape) * (-100)\n",
    "        min_x3 = torch.ones(x.shape) * (-100)\n",
    "        \n",
    "        for i in range(pos.shape[0]):\n",
    "            x1[i, :, :pos[i][0], :]\n",
    "            min_x1[i, :, :pos[i][0], :]  = 0.0\n",
    "            x2[i, :, pos[i][0]:pos[i][1], :]  = 1.0\n",
    "            min_x2[i, :, pos[i][0]:pos[i][1], :] = 0.0\n",
    "            x3[i, :, pos[i][1]:, :] = 1.0\n",
    "            min_x3[i, :, pos[i][1]:, :] = 0.0\n",
    "            \n",
    "         # 卷积结果*mask得到分段后的卷积向量\n",
    "        x1 *= x\n",
    "        x2 *= x\n",
    "        x3 *= x\n",
    "        \n",
    "        # 将无用部分赋予最小值，避免影响maxpool操作\n",
    "        x1 += min_x1\n",
    "        x2 += min_x2\n",
    "        x3 += min_x3\n",
    "        \n",
    "        # 分段池化\n",
    "        x1 = self.maxpool(x1)\n",
    "        x2 = self.maxpool(x2)\n",
    "        x3 = self.maxpool(x3)\n",
    "        \n",
    "        # 展平\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        x3 = x3.view(x3.size(0), -1)\n",
    "        \n",
    "        x = torch.cat((x1,x2,x3),dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Global_TEXTCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Global_TEXTCNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(  # input shape (1, BATCH_SIZE, 200)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=(1,256),\n",
    "            ), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(50,1)), \n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential( \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=(3,256), \n",
    "            ), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=(48,1)), \n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential( \n",
    "            nn.Conv2d(\n",
    "                in_channels=1, \n",
    "                out_channels=16, \n",
    "                kernel_size=(5,256), \n",
    "                stride = 1,\n",
    "            ), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(46,1)),\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Linear(48, LABELNUMS-1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape((-1,1,x.shape[1],x.shape[2]))\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        x3 = self.conv3(x)\n",
    "        x = torch.cat((x1,x2,x3),1)\n",
    "        x = x.view(x.size(0), -1)   # 展平\n",
    "#         x = self.out(x)\n",
    "        output = self.output(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型示意图\n",
    "![](rank.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "class Rank_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Rank_model, self).__init__()\n",
    "        # Global_Model\n",
    "        self.Global_Model = Global_TEXTCNN()\n",
    "        \n",
    "        # Encoder\n",
    "        self.Encoder = Encoder()\n",
    "        self.maxpool = torch.nn.MaxPool2d((SENTEN_LEN,1))\n",
    "        \n",
    "        # NA-人物关系 <NA=0>\n",
    "        self.model1 = Bi_LSTM_max() \n",
    "        self.model1_Liner = nn.Linear(256,2)\n",
    "        \n",
    "        # 人物关系/亲属关系-社交关系-师生关系\n",
    "        self.model2 = Bi_LSTM_max() \n",
    "        self.model2_Liner = nn.Linear(256,3)\n",
    "        \n",
    "        # 亲属关系/配偶-血亲-姻亲\n",
    "        self.model3 = Bi_LSTM_max() \n",
    "        self.model3_Liner = nn.Linear(256,3)\n",
    "        \n",
    "        # 社交关系/友谊关系-感情关系  <友谊关系/朋友=30>\n",
    "        self.model4 = Bi_LSTM_max()\n",
    "        self.model4_Liner = nn.Linear(256,2)\n",
    "        \n",
    "        # 师生关系/老师-学生 <叶子结点使用全联接层输出33, 34>\n",
    "        self.Teacher_Student_CNN = PCNN()\n",
    "        self.Teacher_Student = nn.Linear(150,2)\n",
    "        \n",
    "        # 配偶/丈夫-妻子\n",
    "        self.model5 = Bi_LSTM_max()\n",
    "        self.model5_Liner = nn.Linear(256,2)\n",
    "        \n",
    "        # 血亲/自然血亲/祖父母-外祖父母-父母-子女-孙子女-兄弟姐妹-叔伯姑舅姨-侄甥 <外祖父母/外公=9>\n",
    "        self.model6 = Bi_LSTM_max()\n",
    "        self.model6_Liner = nn.Linear(256,8)\n",
    "        \n",
    "        # 姻亲/血亲的配偶-配偶的血亲\n",
    "        self.model7 = Bi_LSTM_max()\n",
    "        self.model7_Liner = nn.Linear(256,2)\n",
    "        \n",
    "        # 恋人-喜欢  <叶子结点使用全联接层输出 31, 32>\n",
    "        self.Love_Like_CNN = PCNN()\n",
    "        self.Love_Like = nn.Linear(150,2)\n",
    "        \n",
    "        # 丈夫/现夫-前夫-未婚夫 <叶子结点使用全联接层输出1, 2, 3>\n",
    "        self.now_pre_un_man_CNN = PCNN()\n",
    "        self.now_pre_un_man = nn.Linear(150,3)\n",
    "        \n",
    "        # 妻子/现妻-前妻-未婚妻 <叶子结点使用全联接层输出4, 5, 6>\n",
    "        self.now_pre_un_woman_CNN = PCNN()\n",
    "        self.now_pre_un_woman = nn.Linear(150,3)\n",
    "        \n",
    "        # 血亲的配偶/子女的配偶-兄弟姐妹的配偶 <兄弟姐妹的配偶/嫂子=27>\n",
    "        self.model8 = Bi_LSTM_max()\n",
    "        self.model8_Liner = nn.Linear(256,2)\n",
    "        \n",
    "        # 配偶的血亲/配偶的父母/公公-岳父 <叶子结点使用全联接层输出28, 29>\n",
    "        self.father_in_law_CNN = PCNN()\n",
    "        self.father_in_law = nn.Linear(150,2)\n",
    "        \n",
    "        # 祖父母/爷爷-奶奶 <叶子结点使用全联接层输出7, 8>\n",
    "        self.grandparents_CNN = PCNN()\n",
    "        self.grandparents = nn.Linear(150,2)\n",
    "        \n",
    "        # 父母/生父-生母 <叶子结点使用全联接层输出10, 11>\n",
    "        self.parents_CNN = PCNN()\n",
    "        self.parents = nn.Linear(150,2)\n",
    "        \n",
    "        # 子女/儿子-女儿 <叶子结点使用全联接层输出12, 13>\n",
    "        self.son_daughter_CNN = PCNN()\n",
    "        self.son_daughter = nn.Linear(150,2)\n",
    "        \n",
    "        # 孙子女/孙子-孙女  <叶子结点使用全联接层输出14, 15>\n",
    "        self.grandson_granddaughter_CNN = PCNN()\n",
    "        self.grandson_granddaughter = nn.Linear(150,2)\n",
    "        \n",
    "        # 兄弟姐妹/哥哥-弟弟-姐姐-妹妹  <叶子结点使用全联接层输出16, 17, 18, 19>\n",
    "        self.brothers_sisters_CNN = PCNN()\n",
    "        self.brothers_sisters = nn.Linear(150,4)\n",
    "        \n",
    "        # 叔伯姑舅姨/叔伯-舅舅-姑妈 <叶子结点使用全联接层输出20, 21, 22>\n",
    "        self.uncles_aunts_CNN = PCNN()\n",
    "        self.uncles_aunts = nn.Linear(150,3)\n",
    "        \n",
    "        # 侄甥/侄子-侄女 <叶子结点使用全联接层输出23, 24>\n",
    "        self.nephew_niece_CNN = PCNN()\n",
    "        self.nephew_niece = nn.Linear(150,2)\n",
    "        \n",
    "        # 子女的配偶/儿媳-女婿 <叶子结点使用全联接层输出25, 26>\n",
    "        self.daughter_son_in_law_CNN = PCNN()\n",
    "        self.daughter_son_in_law = nn.Linear(150,2)\n",
    "        \n",
    "#         self.loss_Weight = nn.Linear(21,1)\n",
    "        self.sigmod = nn.Sigmoid()\n",
    "    \n",
    "        self.LAMDA = torch.autograd.Variable(torch.Tensor([0.5]) ,requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        x_Encode = self.Encoder(x)\n",
    "        \n",
    "        # NA-人物关系 <NA=0>\n",
    "        after_Model1 = self.model1(x_Encode)\n",
    "        after_Model1_max = self.maxpool(after_Model1)\n",
    "        after_Model1_max = after_Model1_max.reshape(after_Model1_max.shape[0],-1)\n",
    "        after_classify_model1 = self.model1_Liner(after_Model1_max)\n",
    "        y_NA = ((self.sigmod(after_classify_model1[:,0]) >= CLASSIFY_RATE).nonzero()) # 0\n",
    "        \n",
    "        # 人物关系/亲属关系-社交关系-师生关系\n",
    "        model2_Input = after_Model1\n",
    "        after_Model2 = self.model2(model2_Input)\n",
    "        after_Model2_max = self.maxpool(after_Model2)\n",
    "        after_Model2_max = after_Model2_max.reshape(after_Model2_max.shape[0],-1)\n",
    "\n",
    "        # 亲属关系/配偶-血亲-姻亲\n",
    "        model3_Input = after_Model2\n",
    "        after_Model3 = self.model3(model3_Input)\n",
    "\n",
    "        # 社交关系/友谊关系-感情关系  <友谊关系/朋友=30>\n",
    "        model4_Input = after_Model2\n",
    "        after_Model4 = self.model4(model4_Input)\n",
    "        after_Model4_max = self.maxpool(after_Model4)\n",
    "        after_Model4_max = after_Model4_max.reshape(after_Model4_max.shape[0],-1)\n",
    "        after_classify_model4 = self.model4_Liner(after_Model4_max)\n",
    "        y_Firend = ((self.sigmod(after_classify_model4[:, 0]) >= CLASSIFY_RATE).nonzero()) # 3\n",
    "\n",
    "        # 师生关系/老师-学生 <叶子结点使用全联接层输出33, 34>\n",
    "        Teacher_Student_Input = after_Model2\n",
    "        Teacher_Student_Input = self.Teacher_Student_CNN(Teacher_Student_Input, x[:, -2:])\n",
    "        after_classify_Teacher_Student = self.Teacher_Student(Teacher_Student_Input)\n",
    "        y_Teacher = ((self.sigmod(after_classify_Teacher_Student[:, 0]) >= CLASSIFY_RATE).nonzero()) # 33\n",
    "        y_Student = ((self.sigmod(after_classify_Teacher_Student[:, 1]) >= CLASSIFY_RATE).nonzero()) # 34\n",
    "\n",
    "        # 配偶/丈夫-妻子\n",
    "        model5_Input = after_Model3\n",
    "        after_Model5 = self.model5(model5_Input)\n",
    "        after_Model5_max = self.maxpool(after_Model5)\n",
    "        after_Model5_max = after_Model5_max.reshape(after_Model5_max.shape[0],-1)\n",
    "        after_classify_model5 = self.model5_Liner(after_Model5_max)\n",
    "\n",
    "        # 血亲/自然血亲/祖父母-外祖父母-父母-子女-孙子女-兄弟姐妹-叔伯姑舅姨-侄甥 <外祖父母/外公=9>\n",
    "        model6_Input = after_Model3\n",
    "        after_Model6 = self.model6(model6_Input)\n",
    "        after_Model6_max = self.maxpool(after_Model6)\n",
    "        after_Model6_max = after_Model6_max.reshape(after_Model6_max.shape[0],-1)\n",
    "        after_classify_model6 = self.model6_Liner(after_Model6_max)\n",
    "        y_grandpa = ((self.sigmod(after_classify_model6[:, 1]) >= CLASSIFY_RATE).nonzero()) # 9\n",
    "\n",
    "        # 姻亲/血亲的配偶-配偶的血亲\n",
    "        model7_Input = after_Model3\n",
    "        after_Model7 = self.model7(model7_Input)\n",
    "        after_Model7_max = self.maxpool(after_Model7)\n",
    "        after_Model7_max = after_Model7_max.reshape(after_Model7_max.shape[0],-1)\n",
    "        after_classify_model7 = self.model7_Liner(after_Model7_max)\n",
    "\n",
    "        # 感情关系/恋人-喜欢  <叶子结点使用全联接层输出31, 32>\n",
    "        Love_Like_Input = after_Model4\n",
    "        Love_Like_Input = self.Love_Like_CNN(Love_Like_Input, x[:, -2:])\n",
    "        after_Love_Like = self.Love_Like(Love_Like_Input)\n",
    "        y_Love = ((self.sigmod(after_Love_Like[:,0]) >= CLASSIFY_RATE).nonzero()) # 31\n",
    "        y_Like = ((self.sigmod(after_Love_Like[:,1]) >= CLASSIFY_RATE).nonzero()) # 32\n",
    "\n",
    "        # 丈夫/现夫-前夫-未婚夫 <叶子结点使用全联接层输出1, 2, 3>\n",
    "        now_pre_un_man_Input = after_Model5\n",
    "        now_pre_un_man_Input = self.now_pre_un_man_CNN(now_pre_un_man_Input, x[:, -2:])\n",
    "        after_now_pre_un_man = self.now_pre_un_man(now_pre_un_man_Input)\n",
    "        y_now_man = ((self.sigmod(after_now_pre_un_man[:, 0]) >= CLASSIFY_RATE).nonzero()) # 1\n",
    "        y_pre_man = ((self.sigmod(after_now_pre_un_man[:, 1]) >= CLASSIFY_RATE).nonzero()) # 2\n",
    "        y_un_man = ((self.sigmod(after_now_pre_un_man[:, 2]) >= CLASSIFY_RATE).nonzero()) # 3\n",
    "\n",
    "        # 妻子/现妻-前妻-未婚妻 <叶子结点使用全联接层输出4, 5, 6>\n",
    "        now_pre_un_woman_Input = after_Model5\n",
    "        now_pre_un_woman_Input = self.now_pre_un_woman_CNN(now_pre_un_woman_Input, x[:, -2:])\n",
    "        after_now_pre_un_woman = self.now_pre_un_woman(now_pre_un_man_Input)\n",
    "        y_now_woman = ((self.sigmod(after_now_pre_un_woman[:, 0]) >= CLASSIFY_RATE).nonzero()) # 4\n",
    "        y_pre_woman = ((self.sigmod(after_now_pre_un_woman[:, 1]) >= CLASSIFY_RATE).nonzero()) # 5\n",
    "        y_un_woman = ((self.sigmod(after_now_pre_un_woman[:, 2]) >= CLASSIFY_RATE).nonzero()) # 6\n",
    "\n",
    "        # 血亲的配偶/子女的配偶-兄弟姐妹的配偶 <兄弟姐妹的配偶/嫂子=27>\n",
    "        model8_Input = after_Model7\n",
    "        after_Model8 = self.model8(model8_Input)\n",
    "        after_Model8_max = self.maxpool(after_Model8)\n",
    "        after_Model8_max = after_Model8_max.reshape(after_Model8_max.shape[0],-1)\n",
    "        after_classify_model8 = self.model8_Liner(after_Model8_max)\n",
    "        y_sister_in_law = ((self.sigmod(after_classify_model8[:, 1]) >= CLASSIFY_RATE).nonzero()) # 27\n",
    "\n",
    "        # 配偶的血亲/配偶的父母/公公-岳父 <叶子结点使用全联接层输出28, 29>\n",
    "        father_in_law_Input = after_Model7\n",
    "        father_in_law_Input = self.father_in_law_CNN(father_in_law_Input, x[:, -2:])\n",
    "        after_father_in_law = self.father_in_law(father_in_law_Input)\n",
    "        y_wife_father = ((self.sigmod(after_father_in_law[:, 0]) >= CLASSIFY_RATE).nonzero()) # 28\n",
    "        y_husband_father = ((self.sigmod(after_father_in_law[:, 1]) >= CLASSIFY_RATE).nonzero()) # 29\n",
    "\n",
    "        # 祖父母/爷爷-奶奶 <叶子结点使用全联接层输出7, 8>\n",
    "        grandparents_Input = after_Model6\n",
    "        grandparents_Input = self.grandparents_CNN(grandparents_Input, x[:, -2:])\n",
    "        after_grandparents = self.grandparents(grandparents_Input)\n",
    "        y_grandfather = ((self.sigmod(after_grandparents[:, 0]) >= CLASSIFY_RATE).nonzero()) # 7\n",
    "        y_grandmother = ((self.sigmod(after_grandparents[:, 1]) >= CLASSIFY_RATE).nonzero()) # 8\n",
    "\n",
    "        # 父母/生父-生母 <叶子结点使用全联接层输出10, 11>\n",
    "        grandparents_Input = after_Model6\n",
    "        grandparents_Input = self.grandparents_CNN(grandparents_Input, x[:, -2:])\n",
    "        after_parents = self.parents(grandparents_Input)\n",
    "        y_father = ((self.sigmod(after_parents[:, 0]) >= CLASSIFY_RATE).nonzero()) # 10\n",
    "        y_mother = ((self.sigmod(after_parents[:, 1]) >= CLASSIFY_RATE).nonzero()) # 11\n",
    "\n",
    "        # 子女/儿子-女儿 <叶子结点使用全联接层输出12, 13>\n",
    "        son_daughter_Input = after_Model6\n",
    "        son_daughter_Input = self.son_daughter_CNN(son_daughter_Input, x[:, -2:])\n",
    "        after_son_daughter = self.son_daughter(son_daughter_Input)\n",
    "        y_son = ((self.sigmod(after_son_daughter[:, 0]) >= CLASSIFY_RATE).nonzero()) # 12\n",
    "        y_daughter = ((self.sigmod(after_son_daughter[:, 1]) >= CLASSIFY_RATE).nonzero()) # 13\n",
    "\n",
    "        # 孙子女/孙子-孙女  <叶子结点使用全联接层输出14, 15>\n",
    "        grandson_granddaughter_Input = after_Model6\n",
    "        grandson_granddaughter_Input = self.grandson_granddaughter_CNN(grandson_granddaughter_Input, x[:,-2:])\n",
    "        after_grandson_granddaughter  = self.son_daughter(grandson_granddaughter_Input)\n",
    "        y_grandson = ((self.sigmod(after_grandson_granddaughter[:, 0]) >= CLASSIFY_RATE).nonzero()) # 14\n",
    "        y_granddaughter = ((self.sigmod(after_grandson_granddaughter[:, 1]) >= CLASSIFY_RATE).nonzero()) # 15\n",
    "\n",
    "        # 兄弟姐妹/哥哥-弟弟-姐姐-妹妹  <叶子结点使用全联接层输出16, 17, 18, 19>\n",
    "        brothers_sisters_Input = after_Model6\n",
    "        brothers_sisters_Input = self.brothers_sisters_CNN(brothers_sisters_Input, x[:, -2:])\n",
    "        after_brothers_sisters  = self.brothers_sisters(brothers_sisters_Input)\n",
    "        y_elder_brother = ((self.sigmod(after_brothers_sisters[:, 0]) >= CLASSIFY_RATE).nonzero()) # 16\n",
    "        y_younger_brother  = ((self.sigmod(after_brothers_sisters[:, 1]) >= CLASSIFY_RATE).nonzero()) # 17\n",
    "        y_elder_sister  = ((self.sigmod(after_brothers_sisters[:, 2]) >= CLASSIFY_RATE).nonzero()) # 18\n",
    "        y_younger_sister  = ((self.sigmod(after_brothers_sisters[:, 3]) >= CLASSIFY_RATE).nonzero()) # 19\n",
    "\n",
    "        # 叔伯姑舅姨/叔伯-舅舅-姑妈 <叶子结点使用全联接层输出20, 21, 22>\n",
    "        uncles_aunts_Input = after_Model6\n",
    "        uncles_aunts_Input = self.uncles_aunts_CNN(uncles_aunts_Input, x[:, -2:])\n",
    "        after_uncles_aunts  = self.uncles_aunts(uncles_aunts_Input)\n",
    "        y_father_uncles = ((self.sigmod(after_uncles_aunts[:, 0]) >= CLASSIFY_RATE).nonzero()) # 20\n",
    "        y_mother_uncles  = ((self.sigmod(after_uncles_aunts[:, 1]) >= CLASSIFY_RATE).nonzero()) # 21\n",
    "        y_aunts  = ((self.sigmod(after_uncles_aunts[:, 2]) >= CLASSIFY_RATE).nonzero()) # 22\n",
    "\n",
    "\n",
    "        # 侄甥/侄子-侄女 <叶子结点使用全联接层输出23, 24>\n",
    "        nephew_niece_Input = after_Model6\n",
    "        nephew_niece_Input = self.nephew_niece_CNN(nephew_niece_Input, x[:, -2:])\n",
    "        after_nephew_niece  = self.nephew_niece(nephew_niece_Input)\n",
    "        y_nephew = ((self.sigmod(after_nephew_niece[:, 0]) >= CLASSIFY_RATE).nonzero()) # 23\n",
    "        y_niece  = ((self.sigmod(after_nephew_niece[:, 1]) >= CLASSIFY_RATE).nonzero()) # 24\n",
    "\n",
    "        # 子女的配偶/儿媳-女婿 <叶子结点使用全联接层输出25, 26>\n",
    "        daughter_son_in_law_Input = after_Model8\n",
    "        daughter_son_in_law_Input = self.daughter_son_in_law_CNN(daughter_son_in_law_Input, x[:, -2:])\n",
    "        after_daughter_son_in_law = self.daughter_son_in_law(daughter_son_in_law_Input)\n",
    "        y_daughter_in_law = ((self.sigmod(after_daughter_son_in_law[:, 0]) >= CLASSIFY_RATE).nonzero()) # 25\n",
    "        y_son_inlaw = ((self.sigmod(after_daughter_son_in_law[:, 1]) >= CLASSIFY_RATE).nonzero()) # 26\n",
    "        \n",
    "        # Glable_Model\n",
    "        Global_output = self.Global_Model(x_Encode)\n",
    "        output = [\n",
    "            y_NA, y_now_man, y_pre_man, y_un_man, y_now_woman, y_pre_woman, y_un_woman, y_grandfather, y_grandmother, \n",
    "            y_grandpa, y_father, y_mother, y_son, y_daughter, y_grandson, y_granddaughter, y_elder_brother, y_younger_brother, \n",
    "            y_elder_sister, y_younger_sister, y_father_uncles, y_mother_uncles, y_aunts, y_nephew, y_niece, y_daughter_in_law, \n",
    "            y_son_inlaw, y_sister_in_law, y_wife_father, y_husband_father, y_Firend, y_Love, y_Like, y_Teacher, y_Student\n",
    "        ]\n",
    "        \n",
    "        use = [\n",
    "            after_classify_model1[y_NA.reshape(-1).tolist()][:, 0], \n",
    "            after_now_pre_un_man[y_now_man.reshape(-1).tolist()][:, 0], \n",
    "            after_now_pre_un_man[y_pre_man.reshape(-1).tolist()][:, 1], \n",
    "            after_now_pre_un_man[y_un_man.reshape(-1).tolist()][:, 2],\n",
    "            after_now_pre_un_woman[y_now_woman.reshape(-1).tolist()][:, 0], \n",
    "            after_now_pre_un_woman[y_pre_woman.reshape(-1).tolist()][:, 1], \n",
    "            after_now_pre_un_woman[y_un_woman.reshape(-1).tolist()][:, 2], \n",
    "            after_grandparents[y_grandfather.reshape(-1).tolist()][:, 0],\n",
    "            after_grandparents[y_grandmother.reshape(-1).tolist()][:, 1], \n",
    "            after_classify_model6[y_grandpa.reshape(-1).tolist()][:, 1], \n",
    "            after_parents[y_father.reshape(-1).tolist()][:, 0],\n",
    "            after_parents[y_mother.reshape(-1).tolist()][:, 1],\n",
    "            after_son_daughter[y_son.reshape(-1).tolist()][:, 0],\n",
    "            after_son_daughter[y_daughter.reshape(-1).tolist()][:, 1],\n",
    "            after_grandson_granddaughter[y_grandson.reshape(-1).tolist()][:, 0],\n",
    "            after_grandson_granddaughter[y_granddaughter.reshape(-1).tolist()][:, 1],\n",
    "            after_brothers_sisters[y_elder_brother.reshape(-1).tolist()][:, 0],\n",
    "            after_brothers_sisters[y_younger_brother.reshape(-1).tolist()][:, 1],\n",
    "            after_brothers_sisters[y_elder_sister.reshape(-1).tolist()][:, 2],\n",
    "            after_brothers_sisters[y_younger_sister.reshape(-1).tolist()][:, 3],\n",
    "            after_uncles_aunts[y_father_uncles.reshape(-1).tolist()][:, 0],\n",
    "            after_uncles_aunts[y_mother_uncles.reshape(-1).tolist()][:, 1],\n",
    "            after_uncles_aunts[y_aunts.reshape(-1).tolist()][:, 2],\n",
    "            after_nephew_niece[y_nephew.reshape(-1).tolist()][:, 0],\n",
    "            after_nephew_niece[y_niece.reshape(-1).tolist()][:, 1],\n",
    "            after_daughter_son_in_law[y_daughter_in_law.reshape(-1).tolist()][:, 0],\n",
    "            after_daughter_son_in_law[y_son_inlaw.reshape(-1).tolist()][:, 1],\n",
    "            after_classify_model8[y_sister_in_law.reshape(-1).tolist()][:, 1],\n",
    "            after_father_in_law[y_wife_father.reshape(-1).tolist()][:, 0],\n",
    "            after_father_in_law[y_husband_father.reshape(-1).tolist()][:, 1],\n",
    "            after_classify_model4[y_Firend.reshape(-1).tolist()][:, 0],\n",
    "            after_Love_Like[y_Love.reshape(-1).tolist()][:, 0],\n",
    "            after_Love_Like[y_Like.reshape(-1).tolist()][:, 1],\n",
    "            after_classify_Teacher_Student[y_Teacher.reshape(-1).tolist()][:, 0],\n",
    "            after_classify_Teacher_Student[y_Student.reshape(-1).tolist()][:, 1],\n",
    "        ]\n",
    "        tmp = torch.zeros(Global_output.shape)\n",
    "        # lamda * local + (1-lamda) * gobal\n",
    "        for i, out in enumerate(output):\n",
    "            if out.shape[0] == 0:\n",
    "                continue\n",
    "            concat = torch.LongTensor([i for _ in range(Global_output.shape[0])])\n",
    "            index =  (out.reshape(-1), concat.reshape(-1))\n",
    "            tmp.index_put_(index, use[i])\n",
    "        y = tmp * self.LAMDA + (1-self.LAMDA) * Global_output\n",
    "        return [tmp, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still CPU\n"
     ]
    }
   ],
   "source": [
    "model = Rank_model()\n",
    "if torch.cuda.is_available():\n",
    "    print(\"To GPU\")\n",
    "    train_X = train_X.cuda()\n",
    "    train_ontHot_y = train_ontHot_y.cuda()\n",
    "    dev_X = dev_X.cuda()\n",
    "    dev_ontHot_y = dev_ontHot_y.cuda()\n",
    "    model = model.cuda()\n",
    "else:\n",
    "    print(\"Still CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TensorDataset(train_X, train_ontHot_y)\n",
    "dev = TensorDataset(dev_X, dev_ontHot_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset = train, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 0, \n",
    ")\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dataset = dev, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True, \n",
    "    num_workers = 0, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1840a376c1a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdevy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{epoch}: accuracy:{sum(accs)/len(accs)} loss: {torch.mean(torch.stack(losses))} time: {time.time() - start}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#print(\"flag\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "loss_F = torch.nn.BCELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "for epoch in range(1,EPOCHS): \n",
    "    start = time.time()\n",
    "    losses = []\n",
    "    #print(\"1\")\n",
    "    for step,(x,y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        start = time.time()\n",
    "        output, predict = model(x)\n",
    "        loss = loss_F(sigmoid(predict), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.mean())\n",
    "        break\n",
    "    #print(\"2\")\n",
    "    with torch.no_grad():\n",
    "        accs = []\n",
    "        for step,(devx,devy) in enumerate(dev_loader):\n",
    "            dev_out, dev_pred = model(devx)\n",
    "            dev_pred = sigmoid(dev_pred)\n",
    "            tmp = dev_pred > 0.5\n",
    "            count = 0\n",
    "            for i in range(tmp.shape[0]):\n",
    "                if torch.sum(tmp.int()[i] == devy.int()[i]).tolist() == devy.shape[1]:\n",
    "                    count+=1\n",
    "            acc = count / devy.shape[0]\n",
    "            accs.append(acc)\n",
    "        print(f\"{epoch}: accuracy:{sum(accs)/len(accs)} loss: {torch.mean(torch.stack(losses))} time: {time.time() - start}\")\n",
    "    #print(\"flag\")\n",
    "    #assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
